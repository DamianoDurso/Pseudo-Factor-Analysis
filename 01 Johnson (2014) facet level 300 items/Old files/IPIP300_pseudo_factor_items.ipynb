{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25393b87",
   "metadata": {},
   "source": [
    "# Generate embedding matrices for IPIP-NEO-300 \n",
    "### Using several transformers referred to in the manuscript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae87254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run from command line if needed\n",
    "# pip install tensorflow\n",
    "# pip install tensorflow-hub \n",
    "# pip install tensorflow-hub\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be8051",
   "metadata": {},
   "source": [
    "### Read in the data, take the items (ignore signs) and put them in a dataframe for embedding\n",
    "The file imported is a csv file that has the facet code and name, and then each item every time followed by whether it is positively or negatively keyed. Here we ignore the facet code and name and the item signs, on the basis that we are trying to be parallel to factor analysis of ratings where people will not have seen the facet, code, or item signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c4c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'IPIP300_preprocessed_items.csv'  \n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize a list to store sentences from each row\n",
    "row_sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    # Initialize a list to store sentences for the current row\n",
    "    current_row_sentences = []\n",
    "    \n",
    "    # Iterate over each item/column you're interested in\n",
    "    for item in ['item1', 'item2', 'item3', 'item4', 'item5', 'item6', 'item7', 'item8', 'item9', 'item10']:\n",
    "        if item in data.columns:\n",
    "            # Append the cell content as a sentence for the current row\n",
    "            current_row_sentences.append(str(row[item]))\n",
    "    \n",
    "    # Add the current row's sentences to the main list\n",
    "    row_sentences.append(current_row_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcf2df",
   "metadata": {},
   "source": [
    "### Generate the embedding similarity matrices for the item content\n",
    "This creates scale embeddings for ALL items per scale concatenated in one pop. For item level embedding see other notebooks in folder. At the item level in the other folders, we will use item level raw, and item level where we reverse the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d9e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize nli-distilroberta-base-v2 model\n",
    "model = SentenceTransformer('nli-distilroberta-base-v2')\n",
    "\n",
    "# Initialize a list to store the averaged row embeddings\n",
    "averaged_row_embeddings = []\n",
    "\n",
    "# Assuming row_sentences is defined elsewhere and contains the sentences for each row\n",
    "# Iterate over each row's sentences\n",
    "for sentences in row_sentences:\n",
    "    # Initialize a list to store embeddings for the current row's sentences\n",
    "    cell_embeddings = []\n",
    "    \n",
    "    # Encode each sentence and append its embedding\n",
    "    for sentence in sentences:\n",
    "        cell_embedding = model.encode(sentence)\n",
    "        cell_embeddings.append(cell_embedding)\n",
    "    \n",
    "    # Calculate the average embedding for the current row and append to the list\n",
    "    avg_embedding = np.mean(cell_embeddings, axis=0)\n",
    "    averaged_row_embeddings.append(avg_embedding)\n",
    "\n",
    "# Convert the list of averaged embeddings to a single NumPy array\n",
    "averaged_row_embeddings_np = np.array(averaged_row_embeddings)\n",
    "\n",
    "# Compute cosine similarities using the NumPy array\n",
    "cosine_similarities = util.pytorch_cos_sim(averaged_row_embeddings_np, averaged_row_embeddings_np)\n",
    "\n",
    "# Convert the PyTorch tensor of cosine similarities to a pandas DataFrame\n",
    "cosine_similarities_df = pd.DataFrame(cosine_similarities.numpy())\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "cosine_similarities_df.to_csv('matrix_items_roberta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913211ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
